## 一、序言

​		在系统稳定性或者性能测试中，内存的使用情况是一个很重要的监控点，不管是从资源使用的角度还是从发现内存泄露问题的角度。如果笼统的来看，大概就是两个指标，系统的内存使用率和进程使用的内存。但是现实世界的事情往往没有那么简单，稍微细一点来看其实有很多的科目。本文不是一个全面的关于内存使用的探讨，甚至也不是一个详细的Linux下面进程内存使用情况的分析，尽管这里的实践是基于此的。这里想做的是稍微细节一点的来看Linux下一个进程的内存使用情况，包括栈和堆。首先我们从一个简单的C程序开始。且慢，先说一下我试验的环境。

## 二、栈内存

1. 首先我们从一个简单的C程序开始。且慢，先说一下我试验的环境。

   ```c
   /*
   filename: simple_hello.c
   */
   #include <stdio.h>
   
   int main()
   {
   	int 			i;
   	int 			m	= 409600;
   	int 			n	= 0;
   	int 			x;
   	int 			a[m];
   
   	printf("Assign %d values to a[%d]...\n", n, m);
   
   	for (i = 0; i < n; i++) {
   		a[i]		= 100;
   	}
   
   	printf("Value assigned.\n");
   	scanf("%d", &x);
   	return 0;
   }
   ```
   创建一个静态的数组，长度通过m来控制，然后选择性的给部分或者全部的元素赋值，通过n来控制。好吧，这个一个简单的程序能看出什么呢？那我们一起来看看。在Linux下面，查看一个进程的内存使用我们可以下面的命令来实现，只需把其中的[pid]换成进程实际的pid。

   > ```bash
   > cat /proc/[pid]/status
   > ```
   >
   > 为了方便，我们把查找pid和看内存整合成一条命令，后面这将是我们唯一的测试工具。
   >
   > ```bash
   > cat /proc/`ps -ef|grep hello | grep -v grep | awk '{print $2}'`/status | grep -E 'VmSize|VmRSS|VmData|VmStk|VmExe|VmLib'
   > ```



2. 在这里我们关注VmSize|VmRSS|VmData|VmStk|VmExe|VmLib 这个6个指标，下面有一些简单的解释。

   > **VmSize：** 虚拟内存大小。整个进程使用虚拟内存大小，是VmLib, VmExe, VmData, 和 VmStk的总和
   >
   > **VmRSS：** 虚拟内存驻留集合大小。这是驻留在物理内存的一部分。它没有交换到硬盘。它包括代码，数据和栈
   >
   > **VmData：** 程序数据段的大小（所占虚拟内存的大小），堆使用的虚拟内存
   >
   > **VmStk：** 任务在用户态的栈的大小， 栈使用的虚拟内存
   >
   > **VmExe：** 程序所拥有的可执行虚拟内存的大小，代码段，不包括任务使用的库
   >
   > **VmLib ：**被映像到任务的虚拟内存空间的库的大小



3. 测试一

    **条件：**开始测试，我们固定m的值为409600，相当于400K，因为数组的元素是int型，在我的环境里面是4Byte，所以真个数组的大小为1600KB。m固定化，我们不断调整n的大小，重写编译，执行，然后用上面的命令查看内存的使用情况，这样我们得到了下面这个表格。
   
   | m值固定为409600 | **VmSize** | **VmRSS** | **VmData** | **VmStk** | **VmExe** | **VmLib** |
   | --------------- | ---------- | --------- | ---------- | --------- | --------- | --------- |
   | n=0             | 3976 kB    | 2468 kB   | 176 kB     | 1612 kB   | 4 kB      | 1652 kB   |
   | n=1024          | 3980 kB    | 2464 kB   | 176 kB     | 1616 kB   | 4 kB      | 1652 kB   |
   | n=2048          | 3980 kB    | 2496 kB   | 176 kB     | 1616 kB   | 4 kB      | 1652 kB   |
   | n=20480         | 3976 kB    | 2424 kB   | 176 kB     | 1612 kB   | 4 kB      | 1652 kB   |
   | n=40960         | 3980 kB    | 2440 kB   | 176 kB     | 1616 kB   | 4 kB      | 1652 kB   |
   | n=409600        | 3976 kB    | 2424 kB   | 176 kB     | 1612 kB   | 4 kB      | 1652 kB   |
   
   **结论：**从这里我们可以得到几个信息：
   
   > 1. 静态的数组使用的空间被分配到VmStk，也就是栈区。
   > 2. 在数组没有初始化的时候并没有实际占用虚拟内存，看VmRss，但是整个虚拟内存的大小还是分配了VmSize。



3. 测试二

    **条件：**接下来我们做另一个测试，让n=m，调整m的大小，也就是说调整数组的大小，然后初始化所有的元素，这样我们得到了下面的表。
   
   | m=n的值  | **VmSize** | **VmRSS** | **VmData** | **VmStk** | **VmExe** | **VmLib** |
   | -------- | ---------- | --------- | ---------- | --------- | --------- | --------- |
   | n=1024   | 2496 kB    | 576 kB    | 176 kB     | 132 kB    | 4 kB      | 1652 kB   |
   | n=2048   | 2496 kB    | 508 kB    | 176 kB     | 132 kB    | 4 kB      | 1652 kB   |
   | n=20480  | 2496 kB    | 508 kB    | 176 kB     | 132 kB    | 4 kB      | 1652 kB   |
   | n=40960  | 2536 kB    | 576 kB    | 176 kB     | 172 kB    | 4 kB      | 1652 kB   |
   | n=409600 | 3980 kB    | 2424 kB   | 176 kB     | 1616 kB   | 4 kB      | 1652 kB   |
   
   **结论：**从这个表中，我们可以看出：
   
   > 1.  栈的使用确实和数组的size相关，但是有个起始预分配的大小，应该是编译器的优化。
   > 2. VmRSS和VmSize跟着一起在涨。是跟着在涨，但是有个问题，栈的空间是有限的，通过这个程序或者你查看系统的设置你可以找到上限。在我的这台机器上上限是**8MB**（备注：`ulimit -s` 命令查看是8192，也就是**8MB**），每个进程，所以这里如果m的值大于2048000，就会出segmentation fault的错误。当然你也可以调整系统的设置，比如通过 `ulimit -s 10240` 将上限调为10MB。但是这个终究不能调得很大，因为对系统会有影响。所以编程中太大的静态数组不是有个好主意。



## 三、堆内存

1. 栈的大小限制还是蛮严格的，现在来看看程序可以使用的另一类存储空间，堆（heap）。关于堆和栈的区别，栈区——由编译器自动分配和释放，一般存放函数的参数值、局部变量的值等（速度较快）；堆区——由程序员分配及释放，若程序员不释放，程序结束后可能由OS回收（速度比较慢，而且容易产生内存碎片）。继续我们的实验，考虑到现在很多系统的后台用C++来写，所以我们把测试程序换成C++的。好吧，我承认其实没有太大的区别，只是申请内存的方式不太一样了。

   ```c++
   /*
   filename: simple_hello.cpp
   */
   #include <iostream>
   using namespace std;
   
   int main()
   {
   	cout << "New some space for array, assign value" << endl;
   
   	int m = 409600;
   	int n = 409600;
   	int *p	= new int[m];
   
   	for (int i = 0; i < n; i++) {
   		p[i] = 100;
   	}
   
   	cout << "value assigned." << endl;
   
   	int x;
   	cin >> x; //hold program
   }
   ```
   这个我们使用的是动态的数组，也就是说数组的内容空间是我们显式的通过new通过向系统申请的。测试工具还是上面的命令行。



2. 测试一

    **条件：**开始测试，我们固定m的值为409600，相当于400K。
   
   | m值固定为409600 | **VmSize** | **VmRSS** | **VmData** | **VmStk** | **VmExe** | **VmLib** |
   | --------------- | ---------- | --------- | ---------- | --------- | --------- | --------- |
   | n=0             | 7492 kB    | 1520 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   | n=1024          | 7492 kB    | 1516 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   | n=2048          | 7492 kB    | 1520 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   | n=20480         | 7492 kB    | 1516 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   | n=40960         | 7492 kB    | 3256 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   | n=409600        | 7492 kB    | 4568 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   
   **结论：**一些观察的结果：
   
   > 1. VmData的大小约为1600KB，因为每个元素4Byte，系统还有一些别的使用。
   > 2. n控制有多少数组的元素被初始化，这也影响了VmRSS的大小。整个VmSize的大小并不受初始化范围的影响，这个结果和之前栈的实验中看到的现象很类似，只不过这里换成了VmData。



3. 测试二

    **条件：**接下来我们做另一个测试，让n=m，然后两个一起调整。
   
   | m=n的值  | **VmSize** | **VmRSS** | **VmData** | **VmStk** | **VmExe** | **VmLib** |
   | -------- | ---------- | --------- | ---------- | --------- | --------- | --------- |
   | n=1024   | 5888 kB    | 1516 kB   | 224 kB     | 132 kB    | 4 kB      | 3356 kB   |
   | n=2048   | 5888 kB    | 1520 kB   | 224 kB     | 132 kB    | 4 kB      | 3356 kB   |
   | n=20480  | 6040 kB    | 1512 kB   | 376 kB     | 132 kB    | 4 kB      | 3356 kB   |
   | n=40960  | 6052 kB    | 3212 kB   | 388 kB     | 132 kB    | 4 kB      | 3356 kB   |
   | n=409600 | 7492 kB    | 4548 kB   | 1828 kB    | 132 kB    | 4 kB      | 3356 kB   |
   
   **结论：**从这个表中，我们可以看出：
   
   > 1.  VmData的size在增长，VmRSS也在跟着一起增长。但是VmRSS一开始分配的余量就比较大，所以VmData刚开始的增长并未立即导致VmSize的改变。
   > 1.  VmSize也跟着一起增长，应该的。
   > 1.  在实际的产品代码，特别是后台的Linux服务器程序中，通常会大量的动态申请和释放内存，使用的就是我们这里提到的VmData，堆上的内存。Ok，你知道了，我要说的是memory leak的问题。通过观察VmData和VmRSS，我们能够很明确的察觉内存泄露的问题。



## 四、总结

​		前面在分析栈的时候我们提到了系统对栈的大小有上限，比如我的系统默认是8MB。那么有个问题就是，那么堆呢？嗯，这个部分其实就涉及到操作系统的内存管理的策略和方法，是个很大的问题，推荐看一下《深入理解计算机系统》相关的章节或者关于现代的操作系统的书籍。这里我们简单做了一下实验。

​		我的实验机器的内存还有不到2GB，一个Linux的虚拟机。当我们设置 m = n = 409600000的时候，相当于要申请1.6GB的内存。我们产品status的时候发现VmData > VmRSS, 这在前面m=n的情况下从来没有出现过。

```bash
cat /proc/`ps -ef|grep fuck | grep -v grep | awk '{print $2}'`/status | grep -E 'VmSize|VmRSS|VmData|VmStk|VmExe|VmLib|VmLck|VmHWM'

VmSize:	 1605892 kB
VmLck:	       0 kB
VmHWM:	 1603076 kB
VmRSS:	 1603076 kB
VmData:	 1600228 kB
VmStk:	     132 kB
VmExe:	       4 kB
VmLib:	    3356 kB

MiB Mem :   7915.3 total,    128.9 free,   4633.2 used,   3153.2 buff/cache
MiB Swap:   2048.0 total,   2042.2 free,      5.8 used.   2985.7 avail Mem 
```

​		所以一定是有些事情发生变化了。于是我们看了一下系统的内存，以及SWAP的使用情况。发现系统的物理内存已经快用了，还留了一些给系统，然后开始使用SWAP了，大家知道这里的SWAP其实是磁盘文件。在这种情况下，我们也许可以更容易理解驻留内存的意义，以及为什么内存不够会导致性能的明显下降。

​		通过这几个小的例子，会发现就是内存使用这样一个指标其实背后都会有很多值得去了解和探讨的细节，其实这里谈到的也只是冰山一角。只有了解并理解了这些重要的细节，我们在测试中去评估我们的产品的时候才会更加的准备，更容易发现和定位问题。另外不要忘记实际中的产品比这两个豆腐块程序要复杂得多。



## 参阅

- [Linux内存泄漏查询方法proc/status解释](https://blog.csdn.net/gubenpeiyuan/article/details/42459025)
